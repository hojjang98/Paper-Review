{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c0a7061",
   "metadata": {},
   "source": [
    "# ðŸ“„ Paper Summary: Vision Transformer (ViT)\n",
    "\n",
    "**Title**: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale  \n",
    "**Authors**: Dosovitskiy et al. (Google Research, Brain Team)  \n",
    "**Published in**: ICLR 2021  \n",
    "**Link**: https://arxiv.org/abs/2010.11929  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Day 1 â€“ Abstract & Introduction\n",
    "\n",
    "### ðŸ“Œ Background & Motivation\n",
    "- **CNN dominance in CV**: For over a decade, convolutional neural networks (CNNs) have been the backbone of computer vision tasks (classification, detection, segmentation).  \n",
    "- **Limitations of CNNs**:  \n",
    "  - Strong **inductive biases** (locality, translation equivariance) limit flexibility.  \n",
    "  - Scaling CNNs to very large datasets gives diminishing returns.  \n",
    "- **NLP breakthrough with Transformers**: Transformers in NLP show excellent scalability without handcrafted inductive biases.  \n",
    "- **Core Question**: Can the same Transformer architecture be applied directly to images, without convolutions?\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Core Idea\n",
    "- Represent an image as a **sequence of patches** (analogous to tokens in NLP).  \n",
    "- Flatten each patch, project it into an embedding space, and feed the sequence into a Transformer encoder.  \n",
    "- Add a **[CLS] token** for classification and **positional embeddings** to retain spatial order.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Contributions\n",
    "1. Proposed the **Vision Transformer (ViT)**: a pure Transformer architecture for image recognition.  \n",
    "2. Demonstrated that ViT, when trained on **large-scale data (JFT-300M)**, can **match or outperform CNNs** on ImageNet and related benchmarks.  \n",
    "3. Showed that **scaling laws** from NLP also apply to visionâ€”performance scales predictably with model and data size.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Day 2 â€“ Method & Architecture\n",
    "\n",
    "### 1. Image to Sequence (Patch Embedding)\n",
    "- Input: \\( H \\times W \\times C \\) image, divided into non-overlapping \\( P \\times P \\) patches.  \n",
    "  Number of patches: \\( N = \\frac{HW}{P^2} \\).  \n",
    "- Flatten each patch to a vector \\( x_p^i \\in \\mathbb{R}^{P^2 \\cdot C} \\).  \n",
    "- Linearly project into a \\( D \\)-dimensional embedding:  \n",
    "  \\[\n",
    "  z_0^i = E \\cdot x_p^i, \\quad E \\in \\mathbb{R}^{D \\times (P^2 \\cdot C)}\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Positional Encoding\n",
    "- Since Transformers lack spatial order awareness, **learnable positional embeddings** \\( E_{pos}^i \\) are added:  \n",
    "  \\[\n",
    "  z_0^i = x_p^i E + E_{pos}^i\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Transformer Encoder\n",
    "- Standard Transformer encoder blocks with:  \n",
    "  - **Multi-Head Self-Attention (MSA)**  \n",
    "  - **Feed-Forward MLP**  \n",
    "  - **Residual connections + Layer Normalization**  \n",
    "- Enables **global information exchange** among all patches.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Classification Head\n",
    "- A learnable **[CLS] token** is prepended to the input sequence.  \n",
    "- The final hidden state of [CLS] after the encoder represents the image.  \n",
    "- A linear classifier (MLP + softmax) predicts the class label.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Day 3 â€“ Experiments & Results\n",
    "\n",
    "### 1. Pre-training\n",
    "- Training from scratch on ImageNet is insufficient.  \n",
    "- ViT is **pre-trained on JFT-300M** (300M images, 18k classes).  \n",
    "- Optimization details:  \n",
    "  - Adam with weight decay  \n",
    "  - Linear learning rate warm-up + cosine decay  \n",
    "  - Regularization: dropout, stochastic depth  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Fine-tuning (Transfer Learning)\n",
    "- Pre-trained ViT is fine-tuned on datasets such as **ImageNet, CIFAR-100, and VTAB**.  \n",
    "- Input resolution is increased during fine-tuning (e.g., 224Ã—224 â†’ 384Ã—384).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Comparison with CNNs\n",
    "- Compared against **ResNet** baselines.  \n",
    "- Findings:  \n",
    "  - On **large datasets**, ViT outperforms CNNs.  \n",
    "  - On **small datasets**, CNNs perform better due to stronger inductive biases.  \n",
    "- Conclusion: ViT needs **large-scale pre-training** to perform well.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Scaling Experiments\n",
    "- Tested at different model sizes (**Base, Large, Huge**).  \n",
    "- Performance improves consistently with scale (both data and model size).  \n",
    "- Confirms **scaling laws** observed in NLP hold for vision.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Ablation Studies\n",
    "- **Patch Size**: smaller patches (16Ã—16) improve accuracy; larger (32Ã—32) hurt performance.  \n",
    "- **Depth & Width**: increasing layers and hidden dimensions improves results.  \n",
    "- **Regularization**: dropout, stochastic depth, and label smoothing increase robustness.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Key Takeaways\n",
    "1. ViTâ€™s success depends heavily on **large-scale pre-training**.  \n",
    "2. With sufficient data, ViT **outperforms CNNs**.  \n",
    "3. **Scaling laws** apply to both model and data.  \n",
    "4. Patch size and model capacity significantly affect performance.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Day 4 â€“ Discussion & Conclusion\n",
    "\n",
    "### ðŸ“Œ Strengths of ViT\n",
    "1. **Scalability**  \n",
    "   - Performance scales smoothly with model and dataset size (Section 3.3).  \n",
    "   - Larger models show no saturation, unlike CNNs.  \n",
    "\n",
    "2. **Simplicity**  \n",
    "   - A **pure Transformer encoder** without convolution or handcrafted bias.  \n",
    "   - Demonstrates that general-purpose architectures can achieve state-of-the-art results in vision.  \n",
    "\n",
    "3. **Global Context**  \n",
    "   - Self-attention allows **global information exchange** between all patches, overcoming CNNsâ€™ locality constraint.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Limitations (Explicitly Mentioned)\n",
    "1. **Large Data Requirement**  \n",
    "   - ViT underperforms when trained on small datasets like ImageNet from scratch.  \n",
    "   - Stronger inductive biases in CNNs make them superior in low-data settings.  \n",
    "\n",
    "2. **Compute Demand**  \n",
    "   - Large-scale pre-training on massive datasets (JFT-300M) and high-resolution inputs require substantial compute resources.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Conclusion\n",
    "- ViT proves that **pure Transformer architectures** can outperform CNNs when trained at scale.  \n",
    "- It establishes that **scaling behavior** from NLP generalizes to vision.  \n",
    "- However, ViTâ€™s reliance on large datasets and compute limits its immediate applicability in low-resource settings.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Final Summary\n",
    "| Aspect | Vision Transformer (ViT) |\n",
    "|:--|:--|\n",
    "| Architecture | Pure Transformer Encoder |\n",
    "| Inductive Bias | Minimal (Data-driven) |\n",
    "| Strengths | Scalability, Simplicity, Global Context Modeling |\n",
    "| Weaknesses | Data-hungry, High Compute Demand |\n",
    "| Key Legacy | Introduced Transformers as a scalable alternative to CNNs |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
