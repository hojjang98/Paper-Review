{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c0a7061",
   "metadata": {},
   "source": [
    "# ðŸ“„ Paper Summary: Vision Transformer (ViT)\n",
    "\n",
    "**Title**: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale  \n",
    "**Authors**: Dosovitskiy et al. (Google Research, Brain Team)  \n",
    "**Published in**: ICLR 2021  \n",
    "**Link**: https://arxiv.org/abs/2010.11929  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Day 1 â€“ Abstract & Introduction\n",
    "\n",
    "### ðŸ“Œ Background & Motivation\n",
    "- **CNN dominance in CV**: For over a decade, convolutional neural networks (CNNs) have been the backbone of computer vision tasks (e.g., image classification, detection, segmentation).  \n",
    "- **Limitations of CNNs**:  \n",
    "  - Strong **inductive biases** (locality, translation equivariance) limit flexibility.  \n",
    "  - Scaling CNNs to extremely large datasets shows diminishing returns.  \n",
    "- **NLP breakthrough with Transformers**: In contrast, NLP models based on pure **Transformer architectures** scale extremely well, achieving state-of-the-art results without handcrafted inductive biases.  \n",
    "- **Core Question**: Can the same Transformer architecture that revolutionized NLP be directly applied to vision tasks, without convolutions?  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Core Idea\n",
    "- Represent an image as a **sequence of patches** (like words in NLP).  \n",
    "- Flatten each patch (e.g., 16Ã—16 pixels), project it into an embedding space, and feed it into a standard Transformer encoder.  \n",
    "- Introduce a special **[CLS] token** for classification tasks.  \n",
    "- Add **positional embeddings** to retain spatial information.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Contributions\n",
    "1. Proposed the **Vision Transformer (ViT)**: the first pure Transformer architecture for large-scale image recognition.  \n",
    "2. Demonstrated that ViT, when trained on **large datasets** (JFT-300M), can **match or outperform CNNs** on ImageNet and other benchmarks.  \n",
    "3. Showed that **scaling laws** of Transformers in NLP also apply to vision: performance improves predictably with model/data size.  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## âœ… Day 2 â€“ Method & Architecture\n",
    "\n",
    "### 1. Image to Sequence (Patch Embedding)\n",
    "- Input image size: \\( H \\times W \\times C \\)  \n",
    "- Split the image into **non-overlapping patches** of size \\( P \\times P \\).  \n",
    "  â†’ Total number of patches: \\( N = \\frac{HW}{P^2} \\)  \n",
    "- Flatten each patch into a vector \\( x_p^i \\in \\mathbb{R}^{P^2 \\cdot C} \\).  \n",
    "- Apply a linear projection to map it into a \\( D \\)-dimensional embedding:  \n",
    "  \\[\n",
    "  z_0^i = E \\cdot x_p^i, \\quad E \\in \\mathbb{R}^{D \\times (P^2 \\cdot C)}\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Positional Encoding\n",
    "- Transformers lack inherent order awareness â†’ need **positional information**.  \n",
    "- Add a learnable positional embedding \\( E_{pos}^i \\) to each patch embedding:  \n",
    "  \\[\n",
    "  z_0^i = x_p^i E + E_{pos}^i\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Transformer Encoder\n",
    "- Standard Transformer encoder stack consisting of:  \n",
    "  - **Multi-Head Self-Attention (MSA)**  \n",
    "  - **Feed-Forward Network (MLP block)**  \n",
    "  - Residual connections + Layer Normalization  \n",
    "- This allows modeling of **global relationships** among all patches.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Classification Head\n",
    "- A special **[CLS] token** is prepended to the patch sequence.  \n",
    "- After the Transformer encoder, the final hidden state of [CLS] is taken.  \n",
    "- A classification head (MLP + softmax) is applied to predict the class label.\n",
    "\n",
    "---\n",
    "\n",
    "# âœ… Day 3 â€“ Experiments & Results\n",
    "\n",
    "\n",
    "### 1. Pre-training\n",
    "- ViT does not achieve strong performance when trained only on ImageNet.  \n",
    "- Models are **pre-trained on JFT-300M** (300M images, 18k classes).  \n",
    "- Optimization setup:  \n",
    "  - Adam with weight decay  \n",
    "  - Learning rate: linear warm-up + cosine decay  \n",
    "  - Regularization: dropout, stochastic depth  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Fine-tuning (Transfer Learning)\n",
    "- Pre-trained ViT is fine-tuned on downstream datasets (e.g., **ImageNet, CIFAR-100, VTAB**).  \n",
    "- During fine-tuning, the input resolution is often **increased** for better detail.  \n",
    "  - Example: pre-training at 224Ã—224, fine-tuning at 384Ã—384.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Baseline Comparison\n",
    "- Compared against **ResNet** CNN baselines.  \n",
    "- Key findings:  \n",
    "  - On **large datasets**, ViT **outperforms ResNets**.  \n",
    "  - On **smaller datasets**, ResNets still perform better due to stronger inductive biases.  \n",
    "- Conclusion: ViT requires **large-scale data** to unleash its potential.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Scaling Experiments\n",
    "- ViT was tested at different scales: **Base, Large, Huge**.  \n",
    "- Larger models and more data â†’ consistent performance improvements.  \n",
    "- Confirms that **scaling laws** (observed in NLP Transformers) also hold for vision tasks.  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. Ablation Studies\n",
    "- **Patch Size**:  \n",
    "  - Smaller patches (16Ã—16) yield higher accuracy.  \n",
    "  - Larger patches (32Ã—32) reduce computation but hurt performance.  \n",
    "- **Depth & Width**:  \n",
    "  - Increasing the number of layers and hidden dimensions improves results.  \n",
    "- **Regularization**:  \n",
    "  - Techniques like dropout, stochastic depth, and label smoothing enhance data efficiency.  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Key Takeaways\n",
    "1. ViTâ€™s performance strongly depends on **large-scale pre-training**.  \n",
    "2. With sufficient data and scaling, ViT **surpasses CNNs** on multiple benchmarks.  \n",
    "3. **Scaling laws** from NLP extend to vision tasks.  \n",
    "4. Patch size, model depth/width, and regularization are crucial design factors.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
