{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c0a7061",
   "metadata": {},
   "source": [
    "# ðŸ“„ Paper Summary: Vision Transformer (ViT)\n",
    "\n",
    "**Title**: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale  \n",
    "**Authors**: Dosovitskiy et al. (Google Research, Brain Team)  \n",
    "**Published in**: ICLR 2021  \n",
    "**Link**: https://arxiv.org/abs/2010.11929  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Day 1 â€“ Abstract & Introduction\n",
    "\n",
    "### ðŸ“Œ Background & Motivation\n",
    "- **CNN dominance in CV**: For over a decade, convolutional neural networks (CNNs) have been the backbone of computer vision tasks (e.g., image classification, detection, segmentation).  \n",
    "- **Limitations of CNNs**:  \n",
    "  - Strong **inductive biases** (locality, translation equivariance) limit flexibility.  \n",
    "  - Scaling CNNs to extremely large datasets shows diminishing returns.  \n",
    "- **NLP breakthrough with Transformers**: In contrast, NLP models based on pure **Transformer architectures** scale extremely well, achieving state-of-the-art results without handcrafted inductive biases.  \n",
    "- **Core Question**: Can the same Transformer architecture that revolutionized NLP be directly applied to vision tasks, without convolutions?  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Core Idea\n",
    "- Represent an image as a **sequence of patches** (like words in NLP).  \n",
    "- Flatten each patch (e.g., 16Ã—16 pixels), project it into an embedding space, and feed it into a standard Transformer encoder.  \n",
    "- Introduce a special **[CLS] token** for classification tasks.  \n",
    "- Add **positional embeddings** to retain spatial information.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Contributions\n",
    "1. Proposed the **Vision Transformer (ViT)**: the first pure Transformer architecture for large-scale image recognition.  \n",
    "2. Demonstrated that ViT, when trained on **large datasets** (JFT-300M), can **match or outperform CNNs** on ImageNet and other benchmarks.  \n",
    "3. Showed that **scaling laws** of Transformers in NLP also apply to vision: performance improves predictably with model/data size.  \n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ“„ Vision Transformer (ViT) â€“ Day 2\n",
    "\n",
    "## ðŸ§  Method / Architecture\n",
    "\n",
    "### 1. Image to Sequence (Patch Embedding)\n",
    "- Input image size: \\( H \\times W \\times C \\)  \n",
    "- Split the image into **non-overlapping patches** of size \\( P \\times P \\).  \n",
    "  â†’ Total number of patches: \\( N = \\frac{HW}{P^2} \\)  \n",
    "- Flatten each patch into a vector \\( x_p^i \\in \\mathbb{R}^{P^2 \\cdot C} \\).  \n",
    "- Apply a linear projection to map it into a \\( D \\)-dimensional embedding:  \n",
    "  \\[\n",
    "  z_0^i = E \\cdot x_p^i, \\quad E \\in \\mathbb{R}^{D \\times (P^2 \\cdot C)}\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Positional Encoding\n",
    "- Transformers lack inherent order awareness â†’ need **positional information**.  \n",
    "- Add a learnable positional embedding \\( E_{pos}^i \\) to each patch embedding:  \n",
    "  \\[\n",
    "  z_0^i = x_p^i E + E_{pos}^i\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Transformer Encoder\n",
    "- Standard Transformer encoder stack consisting of:  \n",
    "  - **Multi-Head Self-Attention (MSA)**  \n",
    "  - **Feed-Forward Network (MLP block)**  \n",
    "  - Residual connections + Layer Normalization  \n",
    "- This allows modeling of **global relationships** among all patches.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Classification Head\n",
    "- A special **[CLS] token** is prepended to the patch sequence.  \n",
    "- After the Transformer encoder, the final hidden state of [CLS] is taken.  \n",
    "- A classification head (MLP + softmax) is applied to predict the class label.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
