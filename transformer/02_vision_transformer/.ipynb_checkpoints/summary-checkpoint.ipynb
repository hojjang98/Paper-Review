{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c0a7061",
   "metadata": {},
   "source": [
    "# ðŸ“„ Paper Summary: Vision Transformer (ViT)\n",
    "\n",
    "**Title**: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale  \n",
    "**Authors**: Dosovitskiy et al. (Google Research, Brain Team)  \n",
    "**Published in**: ICLR 2021  \n",
    "**Link**: https://arxiv.org/abs/2010.11929  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Day 1 â€“ Abstract & Introduction\n",
    "\n",
    "### ðŸ“Œ Background & Motivation\n",
    "- **CNN dominance in CV**: For over a decade, convolutional neural networks (CNNs) have been the backbone of computer vision tasks (e.g., image classification, detection, segmentation).  \n",
    "- **Limitations of CNNs**:  \n",
    "  - Strong **inductive biases** (locality, translation equivariance) limit flexibility.  \n",
    "  - Scaling CNNs to extremely large datasets shows diminishing returns.  \n",
    "- **NLP breakthrough with Transformers**: In contrast, NLP models based on pure **Transformer architectures** scale extremely well, achieving state-of-the-art results without handcrafted inductive biases.  \n",
    "- **Core Question**: Can the same Transformer architecture that revolutionized NLP be directly applied to vision tasks, without convolutions?  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Core Idea\n",
    "- Represent an image as a **sequence of patches** (like words in NLP).  \n",
    "- Flatten each patch (e.g., 16Ã—16 pixels), project it into an embedding space, and feed it into a standard Transformer encoder.  \n",
    "- Introduce a special **[CLS] token** for classification tasks.  \n",
    "- Add **positional embeddings** to retain spatial information.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Contributions\n",
    "1. Proposed the **Vision Transformer (ViT)**: the first pure Transformer architecture for large-scale image recognition.  \n",
    "2. Demonstrated that ViT, when trained on **large datasets** (JFT-300M), can **match or outperform CNNs** on ImageNet and other benchmarks.  \n",
    "3. Showed that **scaling laws** of Transformers in NLP also apply to vision: performance improves predictably with model/data size.  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
