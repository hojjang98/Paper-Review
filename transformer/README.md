# ðŸ”„ Transformers Paper Reviews

This folder contains reviews and implementations of academic papers related to **Transformer architectures**.  
It covers foundational works in attention-based models and their applications across NLP, vision, and multimodal tasks.

## ðŸ“‚ Structure
- `01_attention_is_all_you_need/`  
  - **Paper**: *Attention Is All You Need* (Vaswani et al., 2017)  
  - **Content**: Day-by-day summary (abstract, method, experiments, conclusion), math notes, and code snippets

## ðŸŽ¯ Purpose
The goal of this section is to build a solid understanding of **attention mechanisms** and the evolution of Transformer-based models,  
bridging theoretical math concepts (linear algebra, probability, optimization) with practical implementations.
