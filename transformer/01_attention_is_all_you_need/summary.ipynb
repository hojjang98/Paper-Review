{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd8a4595",
   "metadata": {},
   "source": [
    "# ðŸ“„ Paper Summary: Attention Is All You Need\n",
    "\n",
    "**Title**: Attention Is All You Need  \n",
    "**Link**: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)  \n",
    "**Published in**: NeurIPS 2017  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Day 1 â€“ Abstract & Introduction  \n",
    "\n",
    "### ðŸ“Œ Background & Motivation  \n",
    "- Traditional **sequence transduction models** mainly relied on **RNNs/LSTMs/GRUs** or **CNNs**.  \n",
    "- These models typically adopt an **encoderâ€“decoder architecture**, often enhanced with **attention mechanisms**.  \n",
    "- However, recurrent models suffer from:  \n",
    "  - **Sequential computation** â†’ no parallelization possible  \n",
    "  - **Inefficiency** with long sequences (memory bottlenecks, slower training)  \n",
    "  - High training cost and limited scalability  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Proposed Approach  \n",
    "- Introduces the **Transformer**, a new architecture:  \n",
    "  - **Removes recurrence and convolution entirely**  \n",
    "  - Relies **solely on attention mechanisms** for sequence modeling  \n",
    "- Key benefits:  \n",
    "  - Enables **parallelization across sequence elements**  \n",
    "  - Reduces training time significantly  \n",
    "  - Captures **long-range dependencies** more effectively  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Key Contributions  \n",
    "1. **Attention-only architecture** replacing RNNs and CNNs  \n",
    "2. **Parallelizable computation**, boosting efficiency on GPUs  \n",
    "3. **State-of-the-art translation performance**:  \n",
    "   - WMT 2014 Englishâ†’German: 28.4 BLEU (+2 BLEU improvement)  \n",
    "   - WMT 2014 Englishâ†’French: 41.8 BLEU (new single-model SOTA)  \n",
    "4. **Generalization beyond translation**, e.g., English constituency parsing  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ High-Level Workflow  \n",
    "1. **Traditional models**: sequential hidden state updates â†’ bottlenecked by recurrence  \n",
    "2. **Problem**: sequential nature blocks parallelization, slows training  \n",
    "3. **Transformer**: uses attention to select relevant words from the entire input at once  \n",
    "4. **Outcome**: faster training, better performance, improved generalization  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Early Results  \n",
    "- Achieved SOTA with only **12 hours of training on 8 P100 GPUs**  \n",
    "- Training cost much lower than prior architectures  \n",
    "- Proves that **attention-only design is both efficient and powerful**  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ TL;DR Summary  \n",
    "The Transformer replaces recurrence and convolutions with **pure attention mechanisms**,  \n",
    "achieving **parallelization, faster training, and superior accuracy** across translation and other NLP tasks.  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Day 2 â€“ Encoderâ€“Decoder & Scaled Dot-Product Attention  \n",
    "\n",
    "### ðŸ“Œ Encoderâ€“Decoder Architecture  \n",
    "- Most sequence transduction models adopt an **encoderâ€“decoder structure**.  \n",
    "- **Encoder**: maps input sequence  \n",
    "  $$\n",
    "  (x_1, â€¦, x_n)\n",
    "  $$  \n",
    "  into continuous representations  \n",
    "  $$\n",
    "  z = (z_1, â€¦, z_n).\n",
    "  $$  \n",
    "- **Decoder**: generates output sequence  \n",
    "  $$\n",
    "  (y_1, â€¦, y_m)\n",
    "  $$  \n",
    "  autoregressively, using previously generated tokens as additional input at each step.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Transformer Encoder  \n",
    "- Composed of **N = 6 identical layers**.  \n",
    "- Each layer has two sub-layers:  \n",
    "  1. Multi-Head Self-Attention  \n",
    "  2. Position-wise Feed-Forward Network  \n",
    "- Each sub-layer uses **Residual Connection** + **Layer Normalization**:  \n",
    "  $$\n",
    "  output = LayerNorm(x + Sublayer(x))\n",
    "  $$  \n",
    "- All outputs, including embeddings, share dimension  \n",
    "  $$\n",
    "  d_{model} = 512.\n",
    "  $$  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Transformer Decoder  \n",
    "- Also composed of **N = 6 identical layers**.  \n",
    "- Each layer has three sub-layers:  \n",
    "  1. Masked Multi-Head Self-Attention  \n",
    "  2. Encoderâ€“Decoder Attention  \n",
    "  3. Position-wise Feed-Forward Network  \n",
    "- Key points:  \n",
    "  - Decoder self-attention applies **masking** to block future positions.  \n",
    "  - Output embeddings are **offset by one position** to ensure predictions depend only on past outputs.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Attention: General Definition  \n",
    "- An **attention function** maps a query and a set of keyâ€“value pairs to an output.  \n",
    "- The output is a **weighted sum of values**, where weights are determined by a compatibility function between query and keys.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Scaled Dot-Product Attention  \n",
    "- Inputs:  \n",
    "  - Queries and Keys of dimension  \n",
    "    $$\n",
    "    d_k\n",
    "    $$  \n",
    "  - Values of dimension  \n",
    "    $$\n",
    "    d_v\n",
    "    $$  \n",
    "- Process:  \n",
    "  1. Compute dot products between Query and all Keys  \n",
    "  2. Scale by  \n",
    "     $$\n",
    "     \\sqrt{d_k}\n",
    "     $$  \n",
    "  3. Apply **softmax** to obtain weights  \n",
    "  4. Multiply weights with Values and sum  \n",
    "- Formula:  \n",
    "  $$\n",
    "  Attention(Q, K, V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "  $$  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Additive vs Dot-Product Attention  \n",
    "- **Additive Attention**: uses a small feed-forward network for compatibility.  \n",
    "- **Dot-Product Attention**: uses dot products directly.  \n",
    "- Complexity is similar, but dot-product is much faster and more memory-efficient due to optimized matrix multiplications.  \n",
    "- For large  \n",
    "  $$\n",
    "  d_k\n",
    "  $$  \n",
    "  scaling is crucial to prevent vanishing gradients in softmax.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Key Takeaways (Day 2)  \n",
    "1. Transformer retains an **encoderâ€“decoder structure**, but each block relies solely on self-attention and feed-forward networks.  \n",
    "2. Encoder and Decoder employ **Residual Connections + LayerNorm** for stable training.  \n",
    "3. Attention is essentially **Qâ€“K similarity guiding weighted V aggregation**.  \n",
    "4. Scaled Dot-Product Attention ensures both **computational efficiency** and **training stability**.  \n",
    "\n",
    "---\n",
    "\n",
    "ðŸ‘‰ Next (Day 3): Multi-Head Attention â€“ extending Scaled Dot-Product Attention with multiple parallel heads. \n",
    "\n",
    "## âœ… Day 3 â€“ Multi-Head Attention  \n",
    "\n",
    "### ðŸ“Œ Motivation  \n",
    "- A single attention head may capture only one type of relationship.  \n",
    "- Multi-Head Attention allows the model to **jointly attend to information from different subspaces** at different positions.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Mechanism \n",
    "- Instead of one attention, project Queries, Keys, and Values **h times** with different learned projections:  \n",
    "  - \\( QW_i^Q, KW_i^K, VW_i^V \\)  \n",
    "- Each head computes Scaled Dot-Product Attention independently:  \n",
    "  - \\( head_i = Attention(QW_i^Q, KW_i^K, VW_i^V) \\)  \n",
    "- Concatenate results of all heads and apply a final linear projection:  \n",
    "  - \\( MultiHead(Q, K, V) = Concat(head_1, â€¦, head_h)W^O \\)  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Dimensions  \n",
    "- In the paper: \\( h = 8 \\).  \n",
    "- Each head uses \\( d_k = d_v = d_{model}/h = 64 \\).  \n",
    "- This keeps computation cost roughly the same as a single attention function with dimension 512.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Benefits  \n",
    "1. **Diversity**: different heads focus on different aspects (syntax, semantics, positional cues).  \n",
    "2. **Efficiency**: smaller dimensions per head make computations manageable.  \n",
    "3. **Expressiveness**: concatenating multiple heads enriches the final representation.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Key Takeaways (Day 3)  \n",
    "- Multi-Head Attention = many scaled dot-product attentions in parallel.  \n",
    "- Enables capturing multiple types of dependencies simultaneously.  \n",
    "- Essential for Transformerâ€™s strong performance.  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Day 4 â€“ Feed-Forward Networks & Positional Encoding  \n",
    "\n",
    "### ðŸ“Œ Position-wise Feed-Forward Networks  \n",
    "- Each encoder and decoder layer includes a **fully connected feed-forward network** after attention.  \n",
    "- Structure:  \n",
    "  - \\( FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2 \\)  \n",
    "- Applied **independently to each position**, but with the same parameters across all positions.  \n",
    "- In the paper:  \n",
    "  - Input/output dimension \\( d_{model} = 512 \\)  \n",
    "  - Inner-layer dimension \\( d_{ff} = 2048 \\).  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Positional Encoding  \n",
    "- Transformer has no recurrence/convolution â†’ no inherent order information.  \n",
    "- Solution: **Add positional encoding to input embeddings.**  \n",
    "- Defined using sine and cosine functions at varying frequencies:  \n",
    "  - \\( PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) \\)  \n",
    "  - \\( PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) \\)  \n",
    "- Added to embeddings:  \n",
    "  - \\( z = embedding + PE \\)  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Why sinusoidal?  \n",
    "- Allows extrapolation to sequences longer than those seen during training.  \n",
    "- Provides smooth, continuous encoding of positions.  \n",
    "- Encodes **relative distances** naturally via sine/cosine properties.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Key Takeaways (Day 4)  \n",
    "1. FFN adds **non-linear transformations** at each position, boosting model capacity.  \n",
    "2. Positional encodings provide **sequence order information** in an attention-only model.  \n",
    "3. Sinusoidal design is simple, parameter-free, and generalizes to unseen lengths.  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Day 5 â€“ Training & Results  \n",
    "\n",
    "### ðŸ“Œ Optimizer & Learning Rate Schedule  \n",
    "- **Optimizer**: Adam  \n",
    "  - Î²1 = 0.9, Î²2 = 0.98, Îµ = 10^-9  \n",
    "- **Learning Rate Schedule**: increases during warmup, then decays  \n",
    "  - Formula:  \n",
    "    ```\n",
    "    lrate = d_model^(-0.5) * min(step_num^(-0.5), step_num * warmup_steps^(-1.5))\n",
    "    ```\n",
    "  - Warmup steps = 4000  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Regularization  \n",
    "- **Dropout**: 0.1 (applied to all sub-layers, FFN, and attention weights)  \n",
    "- **Label Smoothing**: Îµ = 0.1  \n",
    "  - Changes target distribution from one-hot to smoothed version  \n",
    "  - Helps prevent overfitting and improves generalization  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Training Details  \n",
    "- **Batch size**: 25,000 tokens  \n",
    "- **Hardware**: 8 Ã— NVIDIA P100 GPUs  \n",
    "- **Training time**: ~12 hours  \n",
    "- **Model sizes**:  \n",
    "  - Base: 65M parameters  \n",
    "  - Big: 213M parameters (d_model=1024, h=16, d_ff=4096)  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Results â€“ Translation Benchmarks  \n",
    "- **WMT 2014 Englishâ†’German**: 28.4 BLEU (+2.0 BLEU over prior SOTA)  \n",
    "- **WMT 2014 Englishâ†’French**: 41.8 BLEU (new single-model SOTA)  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Additional Experiments  \n",
    "- Strong results on **English constituency parsing**  \n",
    "- Demonstrates generalization beyond translation  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Key Takeaways (Day 5)  \n",
    "1. Training stabilized by Adam + warmup/decay schedule  \n",
    "2. Dropout and label smoothing enhance generalization  \n",
    "3. Efficient training: SOTA achieved in just 12 hours  \n",
    "4. Transformer proved to be a **scalable, general-purpose architecture**  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
