{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd8a4595",
   "metadata": {},
   "source": [
    "# ðŸ“„ Paper Summary: Attention Is All You Need\n",
    "\n",
    "**Title**: Attention Is All You Need  \n",
    "**Link**: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)  \n",
    "**Published in**: NeurIPS 2017  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Day 1 â€“ Abstract & Introduction  \n",
    "\n",
    "### ðŸ“Œ Background & Motivation  \n",
    "- Traditional **sequence transduction models** mainly relied on **RNNs/LSTMs/GRUs** or **CNNs**.  \n",
    "- These models typically adopt an **encoderâ€“decoder architecture**, often enhanced with **attention mechanisms**.  \n",
    "- However, recurrent models suffer from:  \n",
    "  - **Sequential computation** â†’ no parallelization possible  \n",
    "  - **Inefficiency** with long sequences (memory bottlenecks, slower training)  \n",
    "  - High training cost and limited scalability  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Proposed Approach  \n",
    "- Introduces the **Transformer**, a new architecture:  \n",
    "  - **Removes recurrence and convolution entirely**  \n",
    "  - Relies **solely on attention mechanisms** for sequence modeling  \n",
    "- Key benefits:  \n",
    "  - Enables **parallelization across sequence elements**  \n",
    "  - Reduces training time significantly  \n",
    "  - Captures **long-range dependencies** more effectively  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Key Contributions  \n",
    "1. **Attention-only architecture** replacing RNNs and CNNs  \n",
    "2. **Parallelizable computation**, boosting efficiency on GPUs  \n",
    "3. **State-of-the-art translation performance**:  \n",
    "   - WMT 2014 Englishâ†’German: 28.4 BLEU (+2 BLEU improvement)  \n",
    "   - WMT 2014 Englishâ†’French: 41.8 BLEU (new single-model SOTA)  \n",
    "4. **Generalization beyond translation**, e.g., English constituency parsing  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ High-Level Workflow  \n",
    "1. **Traditional models**: sequential hidden state updates â†’ bottlenecked by recurrence  \n",
    "2. **Problem**: sequential nature blocks parallelization, slows training  \n",
    "3. **Transformer**: uses attention to select relevant words from the entire input at once  \n",
    "4. **Outcome**: faster training, better performance, improved generalization  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Early Results  \n",
    "- Achieved SOTA with only **12 hours of training on 8 P100 GPUs**  \n",
    "- Training cost much lower than prior architectures  \n",
    "- Proves that **attention-only design is both efficient and powerful**  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ TL;DR Summary  \n",
    "The Transformer replaces recurrence and convolutions with **pure attention mechanisms**,  \n",
    "achieving **parallelization, faster training, and superior accuracy** across translation and other NLP tasks.  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Day 2 â€“ Encoderâ€“Decoder & Scaled Dot-Product Attention  \n",
    "\n",
    "### ðŸ“Œ Encoderâ€“Decoder Architecture  \n",
    "- Most sequence transduction models adopt an **encoderâ€“decoder structure**.  \n",
    "- **Encoder**: maps input sequence  \n",
    "  $$\n",
    "  (x_1, â€¦, x_n)\n",
    "  $$  \n",
    "  into continuous representations  \n",
    "  $$\n",
    "  z = (z_1, â€¦, z_n).\n",
    "  $$  \n",
    "- **Decoder**: generates output sequence  \n",
    "  $$\n",
    "  (y_1, â€¦, y_m)\n",
    "  $$  \n",
    "  autoregressively, using previously generated tokens as additional input at each step.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Transformer Encoder  \n",
    "- Composed of **N = 6 identical layers**.  \n",
    "- Each layer has two sub-layers:  \n",
    "  1. Multi-Head Self-Attention  \n",
    "  2. Position-wise Feed-Forward Network  \n",
    "- Each sub-layer uses **Residual Connection** + **Layer Normalization**:  \n",
    "  $$\n",
    "  output = LayerNorm(x + Sublayer(x))\n",
    "  $$  \n",
    "- All outputs, including embeddings, share dimension  \n",
    "  $$\n",
    "  d_{model} = 512.\n",
    "  $$  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Transformer Decoder  \n",
    "- Also composed of **N = 6 identical layers**.  \n",
    "- Each layer has three sub-layers:  \n",
    "  1. Masked Multi-Head Self-Attention  \n",
    "  2. Encoderâ€“Decoder Attention  \n",
    "  3. Position-wise Feed-Forward Network  \n",
    "- Key points:  \n",
    "  - Decoder self-attention applies **masking** to block future positions.  \n",
    "  - Output embeddings are **offset by one position** to ensure predictions depend only on past outputs.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Attention: General Definition  \n",
    "- An **attention function** maps a query and a set of keyâ€“value pairs to an output.  \n",
    "- The output is a **weighted sum of values**, where weights are determined by a compatibility function between query and keys.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Scaled Dot-Product Attention  \n",
    "- Inputs:  \n",
    "  - Queries and Keys of dimension  \n",
    "    $$\n",
    "    d_k\n",
    "    $$  \n",
    "  - Values of dimension  \n",
    "    $$\n",
    "    d_v\n",
    "    $$  \n",
    "- Process:  \n",
    "  1. Compute dot products between Query and all Keys  \n",
    "  2. Scale by  \n",
    "     $$\n",
    "     \\sqrt{d_k}\n",
    "     $$  \n",
    "  3. Apply **softmax** to obtain weights  \n",
    "  4. Multiply weights with Values and sum  \n",
    "- Formula:  \n",
    "  $$\n",
    "  Attention(Q, K, V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "  $$  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Additive vs Dot-Product Attention  \n",
    "- **Additive Attention**: uses a small feed-forward network for compatibility.  \n",
    "- **Dot-Product Attention**: uses dot products directly.  \n",
    "- Complexity is similar, but dot-product is much faster and more memory-efficient due to optimized matrix multiplications.  \n",
    "- For large  \n",
    "  $$\n",
    "  d_k\n",
    "  $$  \n",
    "  scaling is crucial to prevent vanishing gradients in softmax.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Key Takeaways (Day 2)  \n",
    "1. Transformer retains an **encoderâ€“decoder structure**, but each block relies solely on self-attention and feed-forward networks.  \n",
    "2. Encoder and Decoder employ **Residual Connections + LayerNorm** for stable training.  \n",
    "3. Attention is essentially **Qâ€“K similarity guiding weighted V aggregation**.  \n",
    "4. Scaled Dot-Product Attention ensures both **computational efficiency** and **training stability**.  \n",
    "\n",
    "---\n",
    "\n",
    "ðŸ‘‰ Next (Day 3): Multi-Head Attention â€“ extending Scaled Dot-Product Attention with multiple parallel heads. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
