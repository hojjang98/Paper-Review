{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf99814f",
   "metadata": {},
   "source": [
    "# 📄 Paper Summary: InternVideo2 – Scaling Video Foundation Models for Multimodal Understanding  \n",
    "\n",
    "**Title**: InternVideo2: Scaling Video Foundation Models for Multimodal Understanding  \n",
    "**Authors**: Yujie Zhong, Yinan He, Jinghao Zhou, et al.  \n",
    "**Published in**: CVPR 2024 (Best Paper Honorable Mention)  \n",
    "**Link**: [https://arxiv.org/abs/2403.15377](https://arxiv.org/abs/2403.15377)  \n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Day 1 – Abstract & Introduction  \n",
    "\n",
    "### 📌 Background & Motivation  \n",
    "- Vision-language foundation models (e.g., **CLIP**, **BLIP-2**, **InternVideo v1**) successfully aligned image–text modalities.  \n",
    "- However, **videos** introduce additional complexity: temporal dynamics, long-range dependencies, and multimodal cues (e.g., sound, speech).  \n",
    "- Existing models often separate objectives (contrastive, masked modeling, captioning), leading to inefficiency and limited generalization.  \n",
    "- There is a need for a **scalable, unified video foundation model** that learns from large, multimodal data.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Core Idea  \n",
    "InternVideo2 proposes a **progressive multimodal training framework** integrating:\n",
    "1. **Masked Video Modeling (MVM)** – learn temporal–spatial representation via reconstruction.  \n",
    "2. **Multimodal Contrastive Learning** – align visual, textual, and auditory modalities.  \n",
    "3. **Next-Token Prediction** – enable generative reasoning for tasks like captioning and dialogue.  \n",
    "\n",
    "This unified design builds robust temporal understanding and cross-modal alignment in a single model.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Contributions  \n",
    "1. A **unified multimodal video–language model** trained on **400M video–text pairs**.  \n",
    "2. **Progressive training** combining reconstruction, contrastive, and generative objectives.  \n",
    "3. **Scalable architecture** up to 6B parameters with strong generalization across 60+ benchmarks.  \n",
    "4. Extensive evaluation on **retrieval**, **captioning**, **action recognition**, and **video QA**.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Notes  \n",
    "- Mirrors the “foundation model” trend seen in NLP.  \n",
    "- Demonstrates that scaling data and parameters consistently improves video understanding.  \n",
    "- Extends multimodal learning from image-text to **video–text–audio–speech** domains.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 TL;DR  \n",
    "InternVideo2 = unified video–language–audio model trained progressively on 400M pairs, achieving SOTA in multimodal video understanding.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Day 2 – Architecture & Method  \n",
    "\n",
    "### 📌 Background  \n",
    "To process videos efficiently, InternVideo2 extends ViT-style Transformers into **spatiotemporal and multimodal** architectures, allowing unified representation learning.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Core Components  \n",
    "\n",
    "| Module | Role | Key Mechanism |\n",
    "|---------|------|----------------|\n",
    "| **Spatial–Temporal Backbone (ST-Backbone)** | Encode spatiotemporal tokens | Factorized attention across space & time |\n",
    "| **Cross-Modal Fusion Module (CMFM)** | Fuse video, text, audio, speech | Multimodal attention in shared latent space |\n",
    "| **Hierarchical Temporal Encoder (HTE)** | Handle long-range dynamics | Temporal windowing + hierarchical aggregation |\n",
    "| **Multitask Heads** | Connect tasks to shared backbone | MVM, contrastive, next-token prediction |\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Method  \n",
    "\n",
    "1. **Spatial–Temporal Encoding**  \n",
    "   - Videos are divided into **3D patches** (spatial + temporal cubes).  \n",
    "   - Apply **spatial attention** within each frame and **temporal attention** across frames.  \n",
    "   - Enables scalable long-range modeling with reduced compute.\n",
    "\n",
    "   \\[\n",
    "   x_t = \\text{TemporalAttn}(\\text{SpatialAttn}(x_{t-1}))\n",
    "   \\]\n",
    "\n",
    "2. **Cross-Modal Fusion**  \n",
    "   - Each modality (Video, Text, Audio, Speech) attends to others through **shared embeddings**.  \n",
    "   - Controlled by **gating mechanisms** to balance influences.  \n",
    "   \\[\n",
    "   z = \\text{Gate}(\\text{Attention}(V, T, A, S))\n",
    "   \\]\n",
    "\n",
    "3. **Temporal Hierarchy**  \n",
    "   - Frames are chunked into temporal windows → locally encoded → globally aggregated.  \n",
    "   - Similar to Temporal Pyramid Networks but integrated into ViT backbone.\n",
    "\n",
    "4. **Multitask Heads**  \n",
    "   - **MVM** for masked reconstruction  \n",
    "   - **Contrastive** for multimodal alignment  \n",
    "   - **Next-token prediction** for generation  \n",
    "   - **Action classification** (optional supervised branch)\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Notes  \n",
    "- All tasks share the same backbone → efficient parameter sharing.  \n",
    "- Progressive training (MVM → Contrastive → Generation) improves stability.  \n",
    "- Architecture scales from Base (1B) to Giant (6B) with near-linear gains.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 TL;DR  \n",
    "InternVideo2 unifies space–time–modality modeling with a shared transformer backbone and progressive multitask training.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Day 3 – Pretraining Setup & Datasets  \n",
    "\n",
    "### 📌 Background  \n",
    "Scaling to **hundreds of millions of video–text pairs** is essential for generalization.  \n",
    "InternVideo2 leverages **InternVid-400M**, one of the largest multimodal datasets ever assembled.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Datasets  \n",
    "\n",
    "| Dataset | Type | Size | Key Feature |\n",
    "|----------|------|------|--------------|\n",
    "| **InternVid-400M** | Video–Text | 400M | Core dataset; diverse sources, filtered captions |\n",
    "| **WebVid2.5M** | Video–Text | 2.5M | Natural human actions |\n",
    "| **CC3M / CC12M** | Image–Text | 15M | Extra textual grounding |\n",
    "| **AudioSet** | Video–Audio | 2M | Adds audio modality |\n",
    "| **HowTo100M** | Instructional Video | 100M | Rich temporal and linguistic alignment |\n",
    "\n",
    "**Data diversity** → key to multimodal scalability.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Pretraining Pipeline  \n",
    "\n",
    "**Stage 1 – Masked Video Modeling (MVM)**  \n",
    "- Learns robust spatial–temporal representations.  \n",
    "- Loss:  \n",
    "  \\[\n",
    "  \\mathcal{L}_{MVM} = \\| \\hat{x}_{mask} - x_{mask} \\|_2^2\n",
    "  \\]\n",
    "\n",
    "**Stage 2 – Multimodal Contrastive Learning**  \n",
    "- Aligns video ↔ text ↔ audio ↔ speech.  \n",
    "- CLIP-style loss:  \n",
    "  \\[\n",
    "  \\mathcal{L}_{CL} = -\\log \\frac{\\exp(\\text{sim}(v_i, t_i)/\\tau)}{\\sum_j \\exp(\\text{sim}(v_i, t_j)/\\tau)}\n",
    "  \\]\n",
    "\n",
    "**Stage 3 – Next-Token Prediction**  \n",
    "- Causal decoding for generative reasoning (captioning/dialogue).  \n",
    "  \\[\n",
    "  \\mathcal{L}_{GEN} = -\\sum_t \\log P(x_t|x_{<t})\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Training Configuration  \n",
    "\n",
    "| Setting | Value |\n",
    "|----------|--------|\n",
    "| **Model size** | 1B → 6B parameters |\n",
    "| **Optimizer** | AdamW |\n",
    "| **Learning rate** | 1e-4 (cosine decay) |\n",
    "| **Batch size** | 16K clips |\n",
    "| **Hardware** | 1024 × A100 (80GB) |\n",
    "| **Framework** | DeepSpeed + FlashAttention |\n",
    "| **Training duration** | ≈ 2 months (Giant) |\n",
    "\n",
    "**Efficiency**: FP16/BF16 precision, gradient checkpointing, distributed contrastive memory bank.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Fine-Tuning Strategy  \n",
    "\n",
    "| Task | Dataset | Objective |\n",
    "|------|----------|------------|\n",
    "| Video–Text Retrieval | MSR-VTT, VATEX | Contrastive |\n",
    "| Action Recognition | Kinetics-400, SSv2 | Cross-entropy |\n",
    "| Captioning | MSVD, MSR-VTT | Causal LM |\n",
    "| Audio–Visual QA | NExT-QA, AVQA | Multimodal reasoning |\n",
    "\n",
    "**Tip:** Freeze early spatial layers, fine-tune higher temporal and fusion modules (LoRA optional).\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Notes  \n",
    "- Progressive curriculum improves optimization and generalization.  \n",
    "- Massive, diverse data = strong zero-shot transfer across tasks.  \n",
    "- Demonstrates scaling law: more data + parameters → better performance.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 TL;DR  \n",
    "InternVideo2’s 400M multimodal pretraining combines **MVM + CL + GEN** objectives under a massive unified dataset, enabling superior multimodal generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
