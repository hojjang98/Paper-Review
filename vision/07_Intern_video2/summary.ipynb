{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf99814f",
   "metadata": {},
   "source": [
    "# ğŸ“„ Paper Summary: InternVideo2 â€“ Scaling Video Foundation Models for Multimodal Understanding  \n",
    "\n",
    "**Title**: InternVideo2: Scaling Video Foundation Models for Multimodal Understanding  \n",
    "**Authors**: Yujie Zhong, Yinan He, Jinghao Zhou, et al.  \n",
    "**Published in**: CVPR 2024 (Best Paper Honorable Mention)  \n",
    "**Link**: [https://arxiv.org/abs/2403.15377](https://arxiv.org/abs/2403.15377)  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Day 1 â€“ Abstract & Introduction  \n",
    "\n",
    "### ğŸ“Œ Background & Motivation  \n",
    "- Vision-language foundation models (e.g., **CLIP**, **BLIP-2**, **InternVideo v1**) successfully aligned imageâ€“text modalities.  \n",
    "- However, **videos** introduce additional complexity: temporal dynamics, long-range dependencies, and multimodal cues (e.g., sound, speech).  \n",
    "- Existing models often separate objectives (contrastive, masked modeling, captioning), leading to inefficiency and limited generalization.  \n",
    "- There is a need for a **scalable, unified video foundation model** that learns from large, multimodal data.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Core Idea  \n",
    "InternVideo2 proposes a **progressive multimodal training framework** integrating:\n",
    "1. **Masked Video Modeling (MVM)** â€“ learn temporalâ€“spatial representation via reconstruction.  \n",
    "2. **Multimodal Contrastive Learning** â€“ align visual, textual, and auditory modalities.  \n",
    "3. **Next-Token Prediction** â€“ enable generative reasoning for tasks like captioning and dialogue.  \n",
    "\n",
    "This unified design builds robust temporal understanding and cross-modal alignment in a single model.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Contributions  \n",
    "1. A **unified multimodal videoâ€“language model** trained on **400M videoâ€“text pairs**.  \n",
    "2. **Progressive training** combining reconstruction, contrastive, and generative objectives.  \n",
    "3. **Scalable architecture** up to 6B parameters with strong generalization across 60+ benchmarks.  \n",
    "4. Extensive evaluation on **retrieval**, **captioning**, **action recognition**, and **video QA**.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Notes  \n",
    "- Mirrors the â€œfoundation modelâ€ trend seen in NLP.  \n",
    "- Demonstrates that scaling data and parameters consistently improves video understanding.  \n",
    "- Extends multimodal learning from image-text to **videoâ€“textâ€“audioâ€“speech** domains.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ TL;DR  \n",
    "InternVideo2 = unified videoâ€“languageâ€“audio model trained progressively on 400M pairs, achieving SOTA in multimodal video understanding.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Day 2 â€“ Architecture & Method  \n",
    "\n",
    "### ğŸ“Œ Background  \n",
    "To process videos efficiently, InternVideo2 extends ViT-style Transformers into **spatiotemporal and multimodal** architectures, allowing unified representation learning.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Core Components  \n",
    "\n",
    "| Module | Role | Key Mechanism |\n",
    "|---------|------|----------------|\n",
    "| **Spatialâ€“Temporal Backbone (ST-Backbone)** | Encode spatiotemporal tokens | Factorized attention across space & time |\n",
    "| **Cross-Modal Fusion Module (CMFM)** | Fuse video, text, audio, speech | Multimodal attention in shared latent space |\n",
    "| **Hierarchical Temporal Encoder (HTE)** | Handle long-range dynamics | Temporal windowing + hierarchical aggregation |\n",
    "| **Multitask Heads** | Connect tasks to shared backbone | MVM, contrastive, next-token prediction |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Method  \n",
    "\n",
    "1. **Spatialâ€“Temporal Encoding**  \n",
    "   - Videos are divided into **3D patches** (spatial + temporal cubes).  \n",
    "   - Apply **spatial attention** within each frame and **temporal attention** across frames.  \n",
    "   - Enables scalable long-range modeling with reduced compute.\n",
    "\n",
    "   \\[\n",
    "   x_t = \\text{TemporalAttn}(\\text{SpatialAttn}(x_{t-1}))\n",
    "   \\]\n",
    "\n",
    "2. **Cross-Modal Fusion**  \n",
    "   - Each modality (Video, Text, Audio, Speech) attends to others through **shared embeddings**.  \n",
    "   - Controlled by **gating mechanisms** to balance influences.  \n",
    "   \\[\n",
    "   z = \\text{Gate}(\\text{Attention}(V, T, A, S))\n",
    "   \\]\n",
    "\n",
    "3. **Temporal Hierarchy**  \n",
    "   - Frames are chunked into temporal windows â†’ locally encoded â†’ globally aggregated.  \n",
    "   - Similar to Temporal Pyramid Networks but integrated into ViT backbone.\n",
    "\n",
    "4. **Multitask Heads**  \n",
    "   - **MVM** for masked reconstruction  \n",
    "   - **Contrastive** for multimodal alignment  \n",
    "   - **Next-token prediction** for generation  \n",
    "   - **Action classification** (optional supervised branch)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Notes  \n",
    "- All tasks share the same backbone â†’ efficient parameter sharing.  \n",
    "- Progressive training (MVM â†’ Contrastive â†’ Generation) improves stability.  \n",
    "- Architecture scales from Base (1B) to Giant (6B) with near-linear gains.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ TL;DR  \n",
    "InternVideo2 unifies spaceâ€“timeâ€“modality modeling with a shared transformer backbone and progressive multitask training.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Day 3 â€“ Pretraining Setup & Datasets  \n",
    "\n",
    "### ğŸ“Œ Background  \n",
    "Scaling to **hundreds of millions of videoâ€“text pairs** is essential for generalization.  \n",
    "InternVideo2 leverages **InternVid-400M**, one of the largest multimodal datasets ever assembled.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Datasets  \n",
    "\n",
    "| Dataset | Type | Size | Key Feature |\n",
    "|----------|------|------|--------------|\n",
    "| **InternVid-400M** | Videoâ€“Text | 400M | Core dataset; diverse sources, filtered captions |\n",
    "| **WebVid2.5M** | Videoâ€“Text | 2.5M | Natural human actions |\n",
    "| **CC3M / CC12M** | Imageâ€“Text | 15M | Extra textual grounding |\n",
    "| **AudioSet** | Videoâ€“Audio | 2M | Adds audio modality |\n",
    "| **HowTo100M** | Instructional Video | 100M | Rich temporal and linguistic alignment |\n",
    "\n",
    "**Data diversity** â†’ key to multimodal scalability.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Pretraining Pipeline  \n",
    "\n",
    "**Stage 1 â€“ Masked Video Modeling (MVM)**  \n",
    "- Learns robust spatialâ€“temporal representations.  \n",
    "- Loss:  \n",
    "  \\[\n",
    "  \\mathcal{L}_{MVM} = \\| \\hat{x}_{mask} - x_{mask} \\|_2^2\n",
    "  \\]\n",
    "\n",
    "**Stage 2 â€“ Multimodal Contrastive Learning**  \n",
    "- Aligns video â†” text â†” audio â†” speech.  \n",
    "- CLIP-style loss:  \n",
    "  \\[\n",
    "  \\mathcal{L}_{CL} = -\\log \\frac{\\exp(\\text{sim}(v_i, t_i)/\\tau)}{\\sum_j \\exp(\\text{sim}(v_i, t_j)/\\tau)}\n",
    "  \\]\n",
    "\n",
    "**Stage 3 â€“ Next-Token Prediction**  \n",
    "- Causal decoding for generative reasoning (captioning/dialogue).  \n",
    "  \\[\n",
    "  \\mathcal{L}_{GEN} = -\\sum_t \\log P(x_t|x_{<t})\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Training Configuration  \n",
    "\n",
    "| Setting | Value |\n",
    "|----------|--------|\n",
    "| **Model size** | 1B â†’ 6B parameters |\n",
    "| **Optimizer** | AdamW |\n",
    "| **Learning rate** | 1e-4 (cosine decay) |\n",
    "| **Batch size** | 16K clips |\n",
    "| **Hardware** | 1024 Ã— A100 (80GB) |\n",
    "| **Framework** | DeepSpeed + FlashAttention |\n",
    "| **Training duration** | â‰ˆ 2 months (Giant) |\n",
    "\n",
    "**Efficiency**: FP16/BF16 precision, gradient checkpointing, distributed contrastive memory bank.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Fine-Tuning Strategy  \n",
    "\n",
    "| Task | Dataset | Objective |\n",
    "|------|----------|------------|\n",
    "| Videoâ€“Text Retrieval | MSR-VTT, VATEX | Contrastive |\n",
    "| Action Recognition | Kinetics-400, SSv2 | Cross-entropy |\n",
    "| Captioning | MSVD, MSR-VTT | Causal LM |\n",
    "| Audioâ€“Visual QA | NExT-QA, AVQA | Multimodal reasoning |\n",
    "\n",
    "**Tip:** Freeze early spatial layers, fine-tune higher temporal and fusion modules (LoRA optional).\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Notes  \n",
    "- Progressive curriculum improves optimization and generalization.  \n",
    "- Massive, diverse data = strong zero-shot transfer across tasks.  \n",
    "- Demonstrates scaling law: more data + parameters â†’ better performance.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ TL;DR  \n",
    "InternVideo2â€™s 400M multimodal pretraining combines **MVM + CL + GEN** objectives under a massive unified dataset, enabling superior multimodal generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
