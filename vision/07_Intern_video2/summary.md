# ðŸ“„ Paper Summary: InternVideo2 â€“ Scaling Video Foundation Models for Multimodal Understanding

**Title**: InternVideo2: Scaling Video Foundation Models for Multimodal Understanding  
**Authors**: Yujie Zhong, Yinan He, Jinghao Zhou, et al.  
**Published in**: CVPR 2024 (Best Paper Honorable Mention)  
**Link**: [https://arxiv.org/abs/2403.15377](https://arxiv.org/abs/2403.15377)  

---

## âœ… Day 1 â€“ Abstract & Introduction  

### ðŸ“Œ Background & Motivation  
- Vision-language foundation models like **CLIP**, **BLIP-2**, and **InternVideo (v1)** have achieved great success in bridging visual and textual modalities.  
- However, **videos** pose unique challenges beyond static images:  
  - Temporal dependencies (motion, dynamics)  
  - Long-range context and multi-modal interactions (visual, audio, speech)  
- Existing models often rely on **separate objectives** (contrastive, masked modeling, captioning) without a unified framework.  
- There is a strong need for a **scalable video foundation model** that can integrate these training paradigms effectively.

---

### ðŸ“Œ Core Idea  
InternVideo2 proposes a **progressive multimodal training framework** that combines:  
1. **Masked video modeling (MVM)** â€“ learn spatiotemporal representations by reconstructing masked video tokens.  
2. **Multimodal contrastive learning** â€“ align video, text, audio, and speech features.  
3. **Next-token prediction** â€“ enable contextual understanding and generation capabilities.  

This unified design allows the model to learn both **temporal coherence** and **cross-modal alignment** across large-scale data.

---

### ðŸ“Œ Model Highlights  
- **Unified architecture** integrating spatial-temporal modeling and multimodal fusion.  
- **Scalable pretraining** on 400M video-text pairs from diverse sources.  
- **Hierarchical temporal encoder** for long video understanding.  
- Trained progressively from base â†’ giant (up to **6B parameters**).  
- Supports **video retrieval**, **captioning**, **action recognition**, and **video dialogue** tasks.  

---

### ðŸ“Œ Training Strategy  
- Employs a **progressive curriculum**:  
  1. Start with masked video pretraining for spatial-temporal consistency.  
  2. Add multimodal contrastive alignment (videoâ€“textâ€“audioâ€“speech).  
  3. Conclude with autoregressive next-token prediction for reasoning.  
- Leverages **semantic video segmentation** and **caption fusion** to ensure tight alignment between modalities.  
- Uses large-scale distributed training (DeepSpeed, FlashAttention) for efficiency.  

---

### ðŸ“Œ Main Contributions  
1. Introduces a **unified multimodal video-language foundation model** trained on massive, diverse datasets.  
2. Proposes a **progressive training framework** combining reconstruction, contrastive, and generative objectives.  
3. Demonstrates **state-of-the-art results** on 60+ benchmarks covering retrieval, recognition, and dialogue tasks.  
4. Shows strong **scalability** and **generalization** to long video and audio-visual reasoning.  

---

### ðŸ“Œ Key Observations  
- Scaling video foundation models yields consistent performance gains, mirroring trends in LLMs and CLIP-like systems.  
- Multimodal supervision (especially audio and speech) greatly enhances semantic understanding.  
- The framework generalizes across domains without task-specific finetuning.  

---

### ðŸ“Œ TL;DR  
InternVideo2 = Unified, scalable, and multimodal video foundation model.  
Trained progressively via masked modeling + contrastive alignment + next-token prediction,  
it achieves SOTA across a wide range of video-language tasks.  

---
