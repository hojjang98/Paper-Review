{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed815592",
   "metadata": {},
   "source": [
    "# ðŸ“„ Paper Summary: Denoising Diffusion Probabilistic Models (DDPM)\n",
    "\n",
    "**Title**: Denoising Diffusion Probabilistic Models  \n",
    "**Authors**: Jonathan Ho, Ajay Jain, Pieter Abbeel  \n",
    "**Published in**: NeurIPS 2020  \n",
    "**Link**: [https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239)  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Day 1 â€“ Abstract & Introduction  \n",
    "\n",
    "### ðŸ“Œ Background & Motivation  \n",
    "Deep generative models such as GANs, VAEs, autoregressive models, and flow-based models have produced high-quality results across multiple modalities.  \n",
    "However, each has critical limitations:  \n",
    "- **VAE**: often generates blurry outputs due to variational approximations  \n",
    "- **GAN**: unstable training, prone to mode collapse  \n",
    "- **Flow-based models**: strong inductive biases, complex architectures  \n",
    "\n",
    "These issues motivate the exploration of new approaches for stable and high-quality image generation.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Core Idea  \n",
    "- A **diffusion model** reframes image generation as a **denoising problem**.  \n",
    "- **Forward process**: gradually corrupt data by adding small amounts of Gaussian noise until all signal is destroyed, leaving pure noise.  \n",
    "- **Reverse process**: train a parameterized Markov chain to progressively remove noise, reconstructing data from random noise.  \n",
    "- When using Gaussian noise, the reverse process can also be parameterized as Gaussians, enabling simple neural network training.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Main Contributions  \n",
    "1. **High-quality synthesis**: Achieves image quality comparable to or exceeding GANs.  \n",
    "2. **Stable training**: Avoids adversarial instabilities and mode collapse.  \n",
    "3. **Simple objective**: Learning reduces to predicting added noise with a mean squared error (MSE) loss.  \n",
    "4. **Novel theoretical connection**: Reveals equivalence between diffusion models, denoising score matching, and Langevin dynamics.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Early Results  \n",
    "- **CIFAR-10**: Inception Score of **9.46** and FID of **3.17** (state-of-the-art at the time).  \n",
    "- **LSUN 256Ã—256**: Sample quality on par with **ProgressiveGAN**.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ TL;DR Summary  \n",
    "Diffusion probabilistic models generate data by **adding Gaussian noise step by step** and then **learning to reverse the corruption process**.  \n",
    "They are stable to train, simple to define, and capable of producing **state-of-the-art image samples**, marking them as a powerful alternative to GANs and VAEs.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
