{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed815592",
   "metadata": {},
   "source": [
    "# ðŸ“„ Paper Summary: Denoising Diffusion Probabilistic Models (DDPM)\n",
    "\n",
    "**Title**: Denoising Diffusion Probabilistic Models  \n",
    "**Authors**: Jonathan Ho, Ajay Jain, Pieter Abbeel  \n",
    "**Published in**: NeurIPS 2020  \n",
    "**Link**: [https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239)  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Day 1 â€“ Abstract & Introduction  \n",
    "\n",
    "### ðŸ“Œ Background & Motivation  \n",
    "Deep generative models such as GANs, VAEs, autoregressive models, and flow-based models have produced high-quality results across multiple modalities.  \n",
    "However, each has critical limitations:  \n",
    "- **VAE**: often generates blurry outputs due to variational approximations  \n",
    "- **GAN**: unstable training, prone to mode collapse  \n",
    "- **Flow-based models**: strong inductive biases, complex architectures  \n",
    "\n",
    "These issues motivate the exploration of new approaches for stable and high-quality image generation.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Core Idea  \n",
    "- A **diffusion model** reframes image generation as a **denoising problem**.  \n",
    "- **Forward process**: gradually corrupt data by adding small amounts of Gaussian noise until all signal is destroyed, leaving pure noise.  \n",
    "- **Reverse process**: train a parameterized Markov chain to progressively remove noise, reconstructing data from random noise.  \n",
    "- When using Gaussian noise, the reverse process can also be parameterized as Gaussians, enabling simple neural network training.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Main Contributions  \n",
    "1. **High-quality synthesis**: Achieves image quality comparable to or exceeding GANs.  \n",
    "2. **Stable training**: Avoids adversarial instabilities and mode collapse.  \n",
    "3. **Simple objective**: Learning reduces to predicting added noise with a mean squared error (MSE) loss.  \n",
    "4. **Novel theoretical connection**: Reveals equivalence between diffusion models, denoising score matching, and Langevin dynamics.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Early Results  \n",
    "- **CIFAR-10**: Inception Score of **9.46** and FID of **3.17** (state-of-the-art at the time).  \n",
    "- **LSUN 256Ã—256**: Sample quality on par with **ProgressiveGAN**.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ TL;DR Summary  \n",
    "Diffusion probabilistic models generate data by **adding Gaussian noise step by step** and then **learning to reverse the corruption process**.  \n",
    "They are stable to train, simple to define, and capable of producing **state-of-the-art image samples**, marking them as a powerful alternative to GANs and VAEs.  \n",
    "\n",
    "## âœ… Day 2 â€“ Forward Process (Noising) \n",
    "\n",
    "### ðŸ“Œ Step 1: Markov Chain Definition  \n",
    "The forward diffusion process gradually corrupts data through a Markov chain.  \n",
    "\n",
    "**Equation (1):**  \n",
    "\\[\n",
    "q(x_{1:T}|x_0) = \\prod_{t=1}^T q(x_t|x_{t-1})\n",
    "\\]  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Step 2: One-Step Transition  \n",
    "Each transition is Gaussian with variance schedule $\\beta_t$.  \n",
    "\n",
    "**Equation (2):**  \n",
    "\\[\n",
    "q(x_t | x_{t-1}) = \\mathcal{N}\\big(x_t;\\,\\sqrt{1-\\beta_t}\\, x_{t-1}, \\, \\beta_t I\\big)\n",
    "\\]  \n",
    "\n",
    "- $\\beta_t$: variance schedule (noise amount).  \n",
    "- $\\alpha_t = 1-\\beta_t$  \n",
    "- $\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Step 3: Marginal Distribution of $x_t$  \n",
    "$x_t$ can be sampled directly from $x_0$ and Gaussian noise $\\epsilon \\sim \\mathcal{N}(0,I)$.  \n",
    "\n",
    "**Equation (4):**  \n",
    "\\[\n",
    "q(x_t|x_0) = \\mathcal{N}\\big(x_t; \\sqrt{\\bar{\\alpha}_t}\\, x_0,\\,(1-\\bar{\\alpha}_t)I\\big)\n",
    "\\]  \n",
    "\n",
    "Thus,  \n",
    "\\[\n",
    "x_t = \\sqrt{\\bar{\\alpha}_t}\\, x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\,\\epsilon\n",
    "\\]  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Key Notes  \n",
    "- The forward process is **parameter-free** (no learning required).  \n",
    "- Thanks to Equation (4), $x_t$ can be **sampled directly from $x_0$** with noise.  \n",
    "- At $t=T$, $x_T \\sim \\mathcal{N}(0,I)$, i.e., pure Gaussian noise.  \n",
    "\n",
    "---\n",
    "### ðŸ“Œ TL;DR Summary  \n",
    "\n",
    "Section **2.1 Forward Process** (Equations (1)â€“(4)) defines how data is gradually destroyed into Gaussian noise through a simple, tractable Markov chain.\n",
    "\n",
    "## âœ… Day 3 â€“ Reverse Process (Denoising) \n",
    "\n",
    "### ðŸ“Œ Step 1: Goal of Reverse Diffusion\n",
    "- Forward process: gradually corrupts data into pure Gaussian noise.  \n",
    "- Reverse process: reconstructs data by modeling the conditional probability  \n",
    "  \\[\n",
    "  q(x_{t-1} | x_t)\n",
    "  \\]  \n",
    "- Problem: this distribution is **intractable**, so we need an approximation.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Step 2: Gaussian Approximation\n",
    "- Assume the reverse transition is also Gaussian:  \n",
    "  \\[\n",
    "  p_\\theta(x_{t-1} | x_t) = \\mathcal{N}\\big(x_{t-1}; \\mu_\\theta(x_t,t), \\Sigma_\\theta(x_t,t)\\big)\n",
    "  \\]  \n",
    "- Mean and variance are parameterized by a neural network.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Step 3: Noise Prediction Network\n",
    "- From the forward equation:  \n",
    "  \\[\n",
    "  x_t = \\sqrt{\\bar{\\alpha}_t}\\,x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\,\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,I)\n",
    "  \\]  \n",
    "- Instead of predicting $\\mu_\\theta$ directly, the network $\\epsilon_\\theta(x_t,t)$ is trained to predict the noise $\\epsilon$.  \n",
    "- Input: $(x_t, t)$  \n",
    "- Output: predicted noise $\\epsilon_\\theta(x_t, t)$  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Step 4: Training Objective\n",
    "- Final simplified loss function:  \n",
    "  \\[\n",
    "  L_{\\text{simple}} = \\mathbb{E}_{t,x_0,\\epsilon}\\big[\\, \\|\\epsilon - \\epsilon_\\theta(x_t,t)\\|^2 \\,\\big]\n",
    "  \\]  \n",
    "- Intuition: train the model to **remove the added noise** at each step.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Key Notes\n",
    "- Reverse process is the **learned part** of diffusion models.  \n",
    "- Training is stable since it reduces to a simple MSE objective.  \n",
    "- This denoising formulation is the core mechanism behind DDPM.  \n",
    "\n",
    "---\n",
    "### ðŸ“Œ TL;DR Summary\n",
    "Reverse process = denoising.  \n",
    "$q(x_{t-1}|x_t)$ is intractable, so approximate with a neural network predicting noise.  \n",
    "Learning reduces to minimizing **MSE between true noise and predicted noise**.\n",
    "\n",
    "--- \n",
    "\n",
    "## âœ… Day 4 â€“ Experiments & Results (2025/09/19)\n",
    "\n",
    "### ðŸ“Œ Experimental Setup\n",
    "- **Datasets**: CIFAR-10, LSUN (bedroom, church, cat), CelebA HQ  \n",
    "- **Metrics**:  \n",
    "  - **FID (FrÃ©chet Inception Distance)**  \n",
    "  - **IS (Inception Score)**  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Key Results\n",
    "- **CIFAR-10**:  \n",
    "  - IS = **9.46**  \n",
    "  - FID = **3.17**  \n",
    "  - â†’ State-of-the-art at the time, surpassing or matching GAN-based methods.  \n",
    "\n",
    "- **LSUN 256Ã—256**:  \n",
    "  - Image quality comparable to **ProgressiveGAN**.  \n",
    "\n",
    "- **CelebA HQ**:  \n",
    "  - High-resolution sample generation with competitive quality.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Observations\n",
    "- Generated samples are **sharp and diverse**, without the **mode collapse** often observed in GANs.  \n",
    "- Training remains **stable and straightforward** due to the MSE-based objective.  \n",
    "- Demonstrates that diffusion models can achieve **GAN-level or superior performance** in image synthesis.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ TL;DR Summary\n",
    "DDPM achieves **state-of-the-art FID/IS scores** on CIFAR-10 and competitive results on LSUN and CelebA HQ.  \n",
    "It provides **GAN-quality samples** while avoiding adversarial training instabilities and mode collapse.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
