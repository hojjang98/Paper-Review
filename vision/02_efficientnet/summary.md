# ğŸ“„ Paper Summary: EfficientNet

**Title**: EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks  
**Link**: [https://arxiv.org/abs/1905.11946](https://arxiv.org/abs/1905.11946)  
**Authors**: Mingxing Tan, Quoc V. Le  
**Published by**: Google AI (ICML 2019)  

---

## âœ… Day 0 â€“ Why I Chose This Paper

### ğŸ“Œ Background & Motivation

After reading **MobileNetV2**, I was curious about what came next in the evolution of efficient CNNs.  
MobileNetV2 introduced **inverted residual blocks** and showed how to design lightweight architectures for mobile and edge devices.

While searching for the next step in the family tree, I discovered **EfficientNet**, which:

- Builds directly on MobileNetV2â€™s design principles  
- Proposes a novel **compound scaling** strategy to scale depth, width, and resolution **together**  
- Uses **Neural Architecture Search (NAS)** to discover a strong baseline model (EfficientNet-B0)  
- Achieves **SOTA accuracy** with significantly fewer FLOPs and parameters

This made it the **perfect follow-up paper** to dive into.

---

## âœ… Day 1 â€“ Abstract, Introduction & Motivation

### ğŸ“Œ Abstract Summary

EfficientNet introduces a new family of CNN models that balance accuracy and efficiency by proposing a **compound scaling method**.  
Instead of scaling network depth, width, or resolution arbitrarily, it scales all three dimensions **uniformly** based on a fixed set of coefficients.  
The base model EfficientNet-B0 is discovered using **Neural Architecture Search**, and larger variants (B1â€“B7) are derived via compound scaling.

EfficientNet models achieve **state-of-the-art accuracy** with **significantly fewer parameters and FLOPs**, making them ideal for both cloud and edge deployment.

---

### ğŸ“Œ Introduction Insights

- Prior CNN models improved accuracy by simply **making networks deeper or wider**, which leads to inefficient computation.  
- EfficientNet solves this by introducing a **principled method** to scale all model dimensions together.  
- Compound scaling maintains a balance across depth, width, and resolution, leading to better accuracy and efficiency.  
- EfficientNet achieves top-1 ImageNet accuracy of **84.4%** with **8.4Ã— fewer FLOPs** and **6.1Ã— fewer parameters** than previous models like GPipe and ResNet.

---

### ğŸ“Œ Problem Statement

Most existing scaling methods focus on only **one** dimension:

- Increase **depth** â†’ may suffer from vanishing gradients  
- Increase **width** â†’ higher memory usage  
- Increase **resolution** â†’ higher computation cost  

There is no systematic rule for scaling them **together**.  
EfficientNet introduces a **compound coefficient Ï†** and a method to uniformly scale all three using constants (Î±, Î², Î³).

---

### ğŸ“Œ Core Design Principles

1. **Search for a good baseline (EfficientNet-B0)** using Neural Architecture Search  
2. **Scale it up** with fixed scaling constants:  

$$
\text{depth} \propto \alpha^\phi,\quad 
\text{width} \propto \beta^\phi,\quad 
\text{resolution} \propto \gamma^\phi
$$

3. Maintain resource constraint:

$$
\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2 \quad \text{(to double FLOPs per } \phi \text{)}
$$

This leads to a family of EfficientNet-B1 to B7.

---

### ğŸ“Œ 3-Line Summary

- EfficientNet proposes a **compound scaling method** that balances depth, width, and resolution.  
- It starts from a **NAS-searched baseline (B0)** and scales uniformly for larger variants.  
- The result is a set of models that are both **accurate and lightweight**, outperforming deeper/wider networks with fewer resources.

---

### ğŸ“Œ Unfamiliar Terms

- **Compound scaling**: A method to jointly scale multiple model dimensions with a single coefficient Ï†.  
- **Neural Architecture Search (NAS)**: An automated way to design neural networks optimized for accuracy and efficiency.  
- **FLOPs**: Floating Point Operations â€” a measure of computational cost.

---

## âœ… TL;DR

EfficientNet presents a **scaling framework** that delivers high accuracy with low cost.  
By searching a compact baseline model and applying **balanced scaling**, it achieves **SOTA performance** with far fewer parameters and FLOPs than previous CNN architectures.  
Its ideas are simple yet powerful, and widely used in modern vision models and real-world deployments.

---

---

## âœ… Day 2 â€“ Motivation Experiments & Compound Scaling

### ğŸ“Œ Motivation by Experiments (Section 3)

EfficientNet experimentally evaluates how scaling only one dimensionâ€”depth, width, or resolutionâ€”affects model performance. The results clearly show **diminishing returns** when scaling is unbalanced.

#### ğŸ“Š Summary of Figure 2

| Subfigure | Experiment            | Summary |
|-----------|------------------------|---------|
| (a)       | Baseline               | EfficientNet-B0 from NAS |
| (b)       | Width only             | Early gains, but performance plateaus quickly |
| (c)       | Depth only             | Gradual improvement, but saturates after a point |
| (d)       | Resolution only        | Accuracy drops at very high input sizes due to noise/overfitting |

#### âš ï¸ Key Insights

- **Depth**: Vanishing gradients, optimization challenges  
- **Width**: Better for fine-grained features, but lacks high-level semantics when overly wide  
- **Resolution**: Too large input â†’ noise and computational overload

> â— Each single-axis scaling shows limited benefit â†’ efficient scaling needs all three dimensions to grow **together**.

---

### ğŸ“Œ Compound Scaling Formula (Section 4.1)

EfficientNet proposes **compound scaling**, a principled way to scale **depth, width, and resolution simultaneously**.

#### ğŸ§® Formula

$$
\text{depth} \propto \alpha^{\phi}, \quad 
\text{width} \propto \beta^{\phi}, \quad 
\text{resolution} \propto \gamma^{\phi}
$$

- **Î±**: depth factor  
- **Î²**: width factor  
- **Î³**: resolution factor  
- **Ï† (phi)**: user-defined compound coefficient â†’ how much to scale the model

#### ğŸ”’ FLOPs Constraint

$$
\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2
$$

- Ensures FLOPs double with each step Ï† â†’ **computational budget grows steadily**
- Î² and Î³ are squared because width and resolution each affect FLOPs **quadratically**

#### ğŸ§  What Ï† Means

- Ï† = 0 â†’ base model (EfficientNet-B0)  
- Ï† = 1 â†’ depth Ã—Î±, width Ã—Î², resolution Ã—Î³  
- Ï† = 2 â†’ depth Ã—Î±Â², etc.  
â†’ FLOPs grow roughly as: $$\text{FLOPs} \approx 2^{\phi}$$

---

### ğŸ’¬ Quick Recap Q&A

- **Q: Why not just scale depth?**  
  A: Gradient vanishing, slow improvements

- **Q: Why not just scale resolution?**  
  A: Too large â†’ noise, memory issues

- **Q: Why use Î±, Î², Î³?**  
  A: To balance scaling across model dimensions with fixed resources

---

## âœ… Day 3 â€“ EfficientNet-B0 Architecture & Scaling Coefficients

### ğŸ“Œ How EfficientNet-B0 Was Designed

EfficientNet-B0 was not manually designed â€” it was discovered using **Neural Architecture Search (NAS)**.  
The search was conducted in the **MnasNet search space**, but with a larger target FLOPs (400M).  
The optimization objective balances accuracy and efficiency:

$$
\text{Objective} = \text{ACC}(M) \cdot \left( \frac{\text{FLOPs}(M)}{T} \right)^w
$$

- \( T = 400M \): FLOPs target  
- \( w = -0.07 \): trade-off weight between accuracy and resource usage

---

### âš™ï¸ Architecture Details: MBConv + SE

EfficientNet-B0 primarily uses **MBConv blocks** (Mobile Inverted Bottlenecks) with **Squeeze-and-Excitation (SE)** modules for better feature calibration.

#### ğŸ”§ Core Elements:
- **MBConv1** used only in early stage (expansion ratio = 1)
- **MBConv6** used elsewhere (expansion ratio = 6)
- **SE modules** improve channel-wise feature importance
- **Skip connections** applied when:
  - Stride = 1  
  - Input and output shapes match

This design keeps the model lightweight yet expressive.

---

### ğŸ“ Compound Coefficients: Î±, Î², Î³

After designing B0, the authors use a **grid search** to find the best scaling coefficients for:

- \( \alpha = 1.2 \): depth  
- \( \beta = 1.1 \): width  
- \( \gamma = 1.15 \): resolution

Subject to the constraint:

$$
\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2
$$

This ensures that **each increment in Ï† roughly doubles FLOPs**.  
Why squared? Because both width and resolution affect computation quadratically.

---

### ğŸ’¡ Why Use Small-Model Search?

Searching (Î±, Î², Î³) on large models is **expensive**.  
So the authors find them using **EfficientNet-B0**, and **reuse them** for all other models (B1â€“B7).  
This keeps the search cost low, while allowing consistent and scalable expansion.

---

### ğŸ“Œ Recap Takeaways

- EfficientNet-B0 is a compact, NAS-optimized architecture using MBConv and SE blocks  
- Compound scaling applies Î±, Î², Î³ to scale all dimensions **uniformly**  
- The constraint \( \alpha \cdot \beta^2 \cdot \gamma^2 \approx 2 \) ensures **predictable FLOPs growth**  
- Coefficients are found once on B0, and reused for larger EfficientNet variants

---

### ğŸ§  Personal Reflection

What stood out today was how **elegant and scalable** the design is.  
Instead of manually tuning every model, EfficientNet finds a good baseline and then grows it using **simple math**.  
This makes it both **theoretically sound and practically efficient** â€” a rare combo in model design.

---


## âœ… Day 4 â€“ Scaling Results (Section 4.3 & Table 2)

### ğŸ“Œ Purpose of This Section

The authors demonstrate that their **compound scaling method** consistently outperforms other popular models across a wide range of sizes â€” from small mobile models to large server-grade models.  
This section validates the **effectiveness and generality** of EfficientNet across different FLOPs budgets.

---

### ğŸ“Š Table 2 Summary: ImageNet Top-1 Accuracy vs. FLOPs

| Model             | Top-1 Accuracy | Params | FLOPs | Notes |
|------------------|----------------|--------|-------|-------|
| ResNet-50        | 76.3%          | 25.6M  | 4.1B  | Strong baseline |
| ResNet-101       | 77.4%          | 44.5M  | 7.8B  | Larger ResNet |
| GPipe-1          | 84.3%          | 556M   | 591B  | Extremely large |
| EfficientNet-B0  | 77.1%          | 5.3M   | 0.39B | Base model |
| EfficientNet-B4  | 83.0%          | 19M    | 4.2B  | Comparable FLOPs to ResNet-50, but much higher accuracy |
| EfficientNet-B7  | **84.4%**      | 66M    | 37B   | SOTA performance |

---

### ğŸ“ˆ Key Takeaways from Scaling Results

- **EfficientNet consistently dominates** in both accuracy and FLOPs/params across all scales.
- Even **EfficientNet-B4**, with only 4.2B FLOPs, outperforms **ResNet-152** (60.2M params, 11.5B FLOPs).
- The **returns start to diminish** around B6â€“B7, meaning accuracy gain becomes smaller despite higher computational cost.
- This confirms the **efficacy of compound scaling** over manual or naive scaling.

---

### ğŸ” At What Point Do Returns Diminish?

- **Diminishing returns** refers to the point where adding more FLOPs or parameters gives **less improvement in accuracy**.
- In EfficientNet, this trend starts appearing **beyond B6**, where the **cost rises faster than the gain**.
- However, the models still remain **more efficient** than existing baselines even at large scales.

---

### ğŸ§  Personal Reflection

Todayâ€™s section showed that **elegant scaling isn't just theoretical** â€” it leads to real-world performance gains.  
It's impressive how EfficientNet can scale up from **tiny mobile models** to **giant server models** using just one method.  
I especially liked how the authors used a **systematic comparison** with popular baselines to prove the generality of their approach.
