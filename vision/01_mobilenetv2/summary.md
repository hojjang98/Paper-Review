# ðŸ“„ Paper Summary: MobileNetV2

**Title**: MobileNetV2: Inverted Residuals and Linear Bottlenecks  
**Link**: [https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)  
**Authors**: Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen  
**Published by**: Google Research (2018)

---

## âœ… Day 1 â€“ Overview & Purpose

### ðŸ“Œ Abstract Summary
MobileNetV2 introduces a lightweight CNN architecture tailored for mobile and resource-constrained environments.  
Its core ideasâ€”**Inverted Residuals** and **Linear Bottlenecks**â€”help reduce computation while maintaining representational power.  
It achieves strong performance on tasks like image classification, object detection, and semantic segmentation, with a balanced trade-off in accuracy, latency, and model size.

---

### ðŸ“Œ Introduction Insights
The paper addresses the inefficiency of existing deep CNNs in mobile applications due to high computation and memory costs.  
To solve this, MobileNetV2 proposes a structure that maintains high accuracy while being computationally efficient.  
Its design expands low-dimensional features to high-dimensional ones, filters them using depthwise convolution, and projects them back linearlyâ€”avoiding large intermediate tensors during inference.

---

### ðŸ“Œ Problem Statement
While modern deep networks achieve superhuman accuracy in tasks like image recognition, their resource demands exceed what mobile and embedded systems can handle.  
This paper proposes a new architecture optimized for such environments.  
The key moduleâ€”**Inverted Residual with Linear Bottleneck**â€”expands inputs to high dimensions, filters them, and compresses back to low dimensions without incurring large memory costs, making it highly efficient for mobile inference.

---

### ðŸ“Œ 3-Line Summary
- Modern deep networks are accurate but unsuitable for mobile devices due to computation and memory constraints.  
- This paper introduces an efficient architecture that retains accuracy while minimizing resource usage.  
- The core design leverages Inverted Residuals and Linear Bottlenecks to reduce intermediate tensor size and computation.

---

### ðŸ“Œ Unfamiliar Terms (None Today)

---

> ðŸ“ Notes:  
> This summary is part of a structured review routine.  
> More in-depth architecture and experiments will be covered in Day 2.

## âœ… Day 2 â€“ Core Architecture Breakdown

### ðŸ“Œ Step 1: Depthwise Separable Convolution
To reduce computation, MobileNetV2 employs **depthwise separable convolutions**, splitting standard convolution into:
- **Depthwise Convolution**: One filter per input channel (spatial filtering).
- **Pointwise Convolution (1Ã—1)**: Linear combinations across channels.

This reduces computation by a factor of **8â€“9Ã—** (when using `3Ã—3` kernels) with only a minor drop in accuracy, making it ideal for mobile devices.

---

### ðŸ“Œ Step 2: Linear Bottlenecks
MobileNetV2 removes non-linearity (ReLU) after the final 1Ã—1 projection layer in the bottleneck block.  
Why? Because **ReLU can destroy information** when applied to low-dimensional manifolds.  
By using a **linear projection**, the architecture preserves essential features and ensures representational capacity is not lost in compressed spaces.

---

### ðŸ“Œ Step 3: Inverted Residuals
Contrary to traditional residual blocks (which connect wide layers), MobileNetV2 **connects narrow bottlenecks via shortcuts**.  
This inverted design:
- Avoids memory-heavy high-dimensional skips
- Preserves gradient flow
- Enables lightweight memory-efficient inference

Residual connections are only used when:
1. **Stride = 1**, and  
2. **Input and output dimensions match**

---

### ðŸ“Œ Step 4: MobileNetV2 Block Structure

A single MobileNetV2 block follows this sequence:

```
Input â†’ 1Ã—1 Conv (Expansion, ReLU6)  
     â†’ 3Ã—3 Depthwise Conv (Stride s, ReLU6)  
     â†’ 1Ã—1 Conv (Projection, **Linear**)  
     â†’ (Optional) Residual connection if stride=1 and input/output dims match
```

- **Expansion factor (t)** is typically `6`
- **ReLU6** is used for non-linearity due to its robustness on low-precision hardware
- The projection layer has no activation (Linear Bottleneck)

This structure is efficient, expressive, and well-suited for mobile inference.

---

### ðŸ“Œ 3-Line Summary
- MobileNetV2 splits convolution into efficient parts and carefully uses activation functions to preserve information.
- Inverted Residuals reduce memory and computation by skipping connections between compressed layers.
- Each block combines expansion, filtering, and projection in a lightweight, modular design.

---

### ðŸ“Œ Unfamiliar Terms
- **Activation manifold**: A lower-dimensional surface where the activation values of a neural network lie.
- **ReLU6**: A capped ReLU activation function: `min(max(0, x), 6)`.

---

> ðŸ“ Notes:  
> Day 3 will explore experimental results, ablations, and comparison with other models like MobileNetV1 and ShuffleNet.

## âœ… Day 3 â€“ Experiments, Applications, and Conclusions

### ðŸ“Œ Step 1: Architecture Details & Expansion

MobileNetV2 uses a repeating bottleneck block structure composed of:

```bash
1Ã—1 Conv (Expansion, ReLU6)
â†’ 3Ã—3 Depthwise Conv (Stride s, ReLU6)
â†’ 1Ã—1 Conv (Linear Projection)
â†’ (+ Residual connection if stride = 1 and input/output dims match)
```

- Expansion factor `t` is typically set to **6**  
- Depthwise convolutions reduce computation drastically  
- Linear projection prevents information loss on low-dimensional manifolds  
- The network uses **19 such blocks** after an initial 3Ã—3 Conv layer  
- **Dropout** and **Batch Normalization** are used during training  
- All kernels are 3Ã—3; width multiplier and input resolution allow scaling

---

### ðŸ“Œ Step 2: Experimental Results

#### ðŸ“ Classification (ImageNet):
| Model | Top-1 Acc | MACs | Params |
|-------|-----------|------|--------|
| MobileNetV1 | 70.6% | 575M | 4.2M |
| **MobileNetV2** | **71.8%** | **300M** | **3.4M** |

- Despite using almost half the computation of V1, V2 achieves better accuracy

#### ðŸ“ Object Detection (COCO, with SSDLite):
| Model | mAP | Latency |
|-------|-----|---------|
| MobileNetV1 + SSD | 19.3 | 27 ms |
| **MobileNetV2 + SSDLite** | **22.1** | **19 ms** |

- V2 achieves **higher accuracy and lower latency**

#### ðŸ“ Memory Usage (Table 3):
- At all spatial resolutions, MobileNetV2 uses **the least memory** (Max = 400K)
- Ideal for low-memory mobile or edge devices

---

### ðŸ“Œ Step 3: Real-World Applications

1. **SSDLite for Object Detection**  
   â†’ Efficient mobile detector with better accuracy and lower latency  
2. **DeepLabv3 for Semantic Segmentation**  
   â†’ Enables real-time segmentation with reduced computational load  
3. **Transfer Learning**  
   â†’ Pretrained MobileNetV2 is widely used across CV tasks and mobile apps

---

### ðŸ“Œ 3-Line Summary

- MobileNetV2 delivers strong performance with minimal computation and memory usage.
- It improves upon V1 by introducing Inverted Residuals and Linear Bottlenecks.
- The architecture proves effective across classification, detection, and segmentation.

---

### ðŸ“Œ Unfamiliar Terms
- **mAP (mean Average Precision)**: Measures detection accuracy across multiple IoU thresholds (COCO uses 0.5â€“0.95).
- **Width Multiplier**: A hyperparameter to scale model width (number of channels).
- **Materialized Memory**: Intermediate tensor storage needed during inference.

---

> ðŸ“ Notes:  
> MobileNetV2 shows that with careful design, deep learning models can be both efficient and accurate.  
> Day 4 may explore successors like ShuffleNetV2 or EfficientNet.



